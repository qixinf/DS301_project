# DS301_project

## Project Title:
Investigating Multimodal Fusion for Emotion Recognition Using Deep Learning

## Introduction:
This project aims to develop a robust multimodal emotion recognition system by combining text (BERT) and audio (MFCC) features.

## Project Structure:
Text Processing: BERT for text-based emotion recognition.

Audio Processing: MFCC for audio feature extraction.

Fusion Model: Combined text and audio features using a fully connected MLP.

## Data Preparation:
The projectâ€™s large data files are hosted externally on Google Drive:
[Google Drive folder link](https://drive.google.com/drive/folders/1nR9BcXhEqyZS_IiCGrG2pRKe0uHG2MtZ?usp=drive_link)

**You do NOT need to manually download the files.**  
All necessary data downloading and file processing is handled automatically by running `code1.ipynb` and `code2.ipynb` in order.

Please ensure your environment has internet access and the required packages installed before running the notebooks.

## Model Training:
Train text model using notebooks/code1.ipynb.

Train fusion model using notebooks/code2.ipynb.
