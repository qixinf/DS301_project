{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f60aa85e93134e65832025186867a59a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eef43d4cfa8143c89cdad6d1a7d9a2bd",
              "IPY_MODEL_df06c73cc40c4fd5a3cfcf841fee03e6",
              "IPY_MODEL_894dd96de0d34dc8be0f860640fc341e"
            ],
            "layout": "IPY_MODEL_eeb699b26c404d5b84d2bb560749cbed"
          }
        },
        "eef43d4cfa8143c89cdad6d1a7d9a2bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad9fcfbe36284e6aaca04ed8b2e62038",
            "placeholder": "​",
            "style": "IPY_MODEL_e2741ff7a0644e72819729570a513999",
            "value": "config.json: 100%"
          }
        },
        "df06c73cc40c4fd5a3cfcf841fee03e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55b9c7ea85874f3bba3a61a377376f59",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d093c63dfcab425dba391c461c3f6937",
            "value": 570
          }
        },
        "894dd96de0d34dc8be0f860640fc341e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61200ff248194adb994be1e886028f52",
            "placeholder": "​",
            "style": "IPY_MODEL_950706d1c5b24d1db1f577b8f9ddbbac",
            "value": " 570/570 [00:00&lt;00:00, 41.8kB/s]"
          }
        },
        "eeb699b26c404d5b84d2bb560749cbed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad9fcfbe36284e6aaca04ed8b2e62038": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2741ff7a0644e72819729570a513999": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55b9c7ea85874f3bba3a61a377376f59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d093c63dfcab425dba391c461c3f6937": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "61200ff248194adb994be1e886028f52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "950706d1c5b24d1db1f577b8f9ddbbac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43679aba79d543c990c3b9d3f386f8dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2e3dd84d73ae493a8fa3635101212287",
              "IPY_MODEL_31e96432f3ac4ea09cb111d993ab7fae",
              "IPY_MODEL_ff94fdc881b64867baffcab2bfe48dce"
            ],
            "layout": "IPY_MODEL_5645ec30925b4128b07107fb199d3c36"
          }
        },
        "2e3dd84d73ae493a8fa3635101212287": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e18a11997c9b4a8aaff05717f2450ba6",
            "placeholder": "​",
            "style": "IPY_MODEL_ae328d6435b544e5bd0ce9dde6479292",
            "value": "model.safetensors: 100%"
          }
        },
        "31e96432f3ac4ea09cb111d993ab7fae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08bc34523970487ab7d64508a716bfd1",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a7ef04493a84f179162fba005d97c6e",
            "value": 440449768
          }
        },
        "ff94fdc881b64867baffcab2bfe48dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a020175ce1e4ef8a93387fb87cf8958",
            "placeholder": "​",
            "style": "IPY_MODEL_c6b855f1e47d4c228fec5b81e410810b",
            "value": " 440M/440M [00:03&lt;00:00, 76.0MB/s]"
          }
        },
        "5645ec30925b4128b07107fb199d3c36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e18a11997c9b4a8aaff05717f2450ba6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae328d6435b544e5bd0ce9dde6479292": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08bc34523970487ab7d64508a716bfd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a7ef04493a84f179162fba005d97c6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9a020175ce1e4ef8a93387fb87cf8958": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6b855f1e47d4c228fec5b81e410810b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bp6XmRjVwH-E",
        "outputId": "9d5df114-6a7e-4198-850a-caa1cf50697d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/DS301-Project\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/DS301-Project"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DS301 Project - Code2 Notebook\n",
        "This notebook extends `code1.ipynb` by adding a **multimodal fusion model** combining **text (BERT)** and **audio (MFCC)** features for emotion classification.\n",
        "\n",
        "We will:\n",
        "1. Load preprocessed text and audio features\n",
        "2. Build a multimodal classifier\n",
        "3. Train and evaluate on MELD dataset\n"
      ],
      "metadata": {
        "id": "IMCh2cJVx8Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import json\n",
        "\n",
        "# Load preprocessed features\n",
        "with open('/content/drive/MyDrive/DS301-Project/meld_text_audio_features.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "# Load label mapping\n",
        "with open('/content/drive/MyDrive/DS301-Project/code/label2id.json', 'r') as f:\n",
        "    label2id = json.load(f)\n",
        "id2label = {v: k for k, v in label2id.items()}"
      ],
      "metadata": {
        "id": "2-iHe8NGyWic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "class MELDMultiModalDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        input_ids = sample['text_tokens']['input_ids'].squeeze(0)\n",
        "        attention_mask = sample['text_tokens']['attention_mask'].squeeze(0)\n",
        "        audio_mfcc = torch.tensor(sample['audio_mfcc'], dtype=torch.float32)\n",
        "        label = label2id[sample['label']]\n",
        "        return input_ids, attention_mask, audio_mfcc, torch.tensor(label)"
      ],
      "metadata": {
        "id": "-7vTXFVpyc0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = [item[0] for item in batch]\n",
        "    attention_masks = [item[1] for item in batch]\n",
        "    audio_mfccs = [item[2] for item in batch]\n",
        "    labels = [item[3] for item in batch]\n",
        "\n",
        "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
        "    attention_masks_padded = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
        "    audio_mfccs_stacked = torch.stack(audio_mfccs)\n",
        "    labels_tensor = torch.stack(labels)\n",
        "\n",
        "    return input_ids_padded, attention_masks_padded, audio_mfccs_stacked, labels_tensor"
      ],
      "metadata": {
        "id": "Xr7oAUasyeDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MELDMultiModalDataset(data)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "_geLZihrygUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultiModalClassifier(nn.Module):\n",
        "    def __init__(self, text_hidden_dim, audio_feat_dim, num_labels):\n",
        "        super().__init__()\n",
        "        self.text_fc = nn.Linear(text_hidden_dim, 128)\n",
        "        self.audio_fc = nn.Linear(audio_feat_dim, 128)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128*2, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, text_emb, audio_feat):\n",
        "        text_out = self.text_fc(text_emb)\n",
        "        audio_out = self.audio_fc(audio_feat)\n",
        "        fused = torch.cat((text_out, audio_out), dim=1)\n",
        "        return self.classifier(fused)"
      ],
      "metadata": {
        "id": "Wpg3pS1Iyj7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "bert.eval()  # freeze BERT initially or allow fine-tuning later"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f60aa85e93134e65832025186867a59a",
            "eef43d4cfa8143c89cdad6d1a7d9a2bd",
            "df06c73cc40c4fd5a3cfcf841fee03e6",
            "894dd96de0d34dc8be0f860640fc341e",
            "eeb699b26c404d5b84d2bb560749cbed",
            "ad9fcfbe36284e6aaca04ed8b2e62038",
            "e2741ff7a0644e72819729570a513999",
            "55b9c7ea85874f3bba3a61a377376f59",
            "d093c63dfcab425dba391c461c3f6937",
            "61200ff248194adb994be1e886028f52",
            "950706d1c5b24d1db1f577b8f9ddbbac",
            "43679aba79d543c990c3b9d3f386f8dc",
            "2e3dd84d73ae493a8fa3635101212287",
            "31e96432f3ac4ea09cb111d993ab7fae",
            "ff94fdc881b64867baffcab2bfe48dce",
            "5645ec30925b4128b07107fb199d3c36",
            "e18a11997c9b4a8aaff05717f2450ba6",
            "ae328d6435b544e5bd0ce9dde6479292",
            "08bc34523970487ab7d64508a716bfd1",
            "7a7ef04493a84f179162fba005d97c6e",
            "9a020175ce1e4ef8a93387fb87cf8958",
            "c6b855f1e47d4c228fec5b81e410810b"
          ]
        },
        "collapsed": true,
        "id": "1bMeuKYnymjb",
        "outputId": "72388a6c-6cd2-4de1-9985-f110dbb5811c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f60aa85e93134e65832025186867a59a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43679aba79d543c990c3b9d3f386f8dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSdpaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next steps:\n",
        "- Pass input_ids & attention_masks through `bert` to get embeddings\n",
        "- Feed embeddings + audio_mfcc to `MultiModalClassifier`\n",
        "- Define optimizer, loss\n",
        "- Training & evaluation loop\n"
      ],
      "metadata": {
        "id": "3Y1gq8JFzV9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = torch.load('input_ids.pt')\n",
        "attention_mask = torch.load('attention_mask.pt')\n",
        "\n",
        "print(input_ids.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlSNUh356_89",
        "outputId": "17d2e31a-279d-43cf-b14d-61a3cdd91223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 44])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pickle\n",
        "\n",
        "with open('/content/drive/MyDrive/DS301-Project/meld_text_audio_features.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "audio_features = torch.stack([torch.tensor(d['audio_mfcc'], dtype=torch.float32).mean(dim=1) for d in data])\n",
        "\n",
        "print(audio_features.shape)  # num_samples, mfcc_dim\n",
        "\n",
        "# Save\n",
        "torch.save(audio_features, '/content/drive/MyDrive/DS301-Project/code/audio_features.pt')\n",
        "print(\"Saved audio_features.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dUCHG7A41I1",
        "outputId": "9a8bb59b-401f-480c-9f2c-34b360c823ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 13])\n",
            "Saved audio_features.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    bert_outputs = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    text_embeddings = bert_outputs.pooler_output\n",
        "\n",
        "# Load audio features\n",
        "audio_features = torch.load('code/audio_features.pt')\n",
        "\n",
        "print(f\"Text embeddings shape: {text_embeddings.shape}\")\n",
        "print(f\"Audio features shape: {audio_features.shape}\")\n",
        "\n",
        "# Concatenate text and audio features\n",
        "fused_features = torch.cat([text_embeddings, audio_features], dim=1)\n",
        "print(f\"Fused features shape: {fused_features.shape}\")"
      ],
      "metadata": {
        "id": "QvEcisg212W0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9021ba6-b978-46ec-ffeb-9d205bedee86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text embeddings shape: torch.Size([100, 768])\n",
            "Audio features shape: torch.Size([100, 13])\n",
            "Fused features shape: torch.Size([100, 781])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define number of labels\n",
        "num_labels = len(label2id)\n",
        "\n",
        "# simple classifier\n",
        "classifier = nn.Linear(768, num_labels)\n",
        "logits = classifier(text_embeddings)\n",
        "\n",
        "# prediction\n",
        "preds = torch.argmax(logits, dim=1)\n",
        "print(preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QU8WeV-T9VvK",
        "outputId": "994fe078-5b69-480c-e6a9-006237f06644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0,\n",
            "        0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load label mapping (needed for num_labels and decoding)\n",
        "with open('/content/drive/MyDrive/DS301-Project/code/label2id.json', 'r') as f:\n",
        "    label2id = json.load(f)\n",
        "\n",
        "# Load data again to get labels\n",
        "with open('/content/drive/MyDrive/DS301-Project/meld_text_audio_features.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "# Extract labels into tensor\n",
        "labels = torch.tensor([label2id[d['label']] for d in data])"
      ],
      "metadata": {
        "id": "Mqyf-2mD-dDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**incorporate audio_features.pt into model**"
      ],
      "metadata": {
        "id": "v3Q_E_rD5W_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_features = torch.load('/content/drive/MyDrive/DS301-Project/code/audio_features.pt')\n",
        "print(audio_features.shape)  # num_samples, mfcc_dim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tZdOg-t5YnH",
        "outputId": "c3c54ca3-7171-4b65-b9a3-30bfe702d3a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 13])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_features = torch.cat([text_embeddings, audio_features], dim=1)  # dim=1 = feature axis\n",
        "print(combined_features.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4upArLL5a00",
        "outputId": "687f051c-af16-4962-a4cc-b33ea2a7883f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 781])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = combined_features.shape[1]\n",
        "classifier = nn.Linear(input_dim, num_labels)"
      ],
      "metadata": {
        "id": "toocsvi35baU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = combined_features.shape[1]\n",
        "classifier = nn.Sequential(\n",
        "    nn.Linear(input_dim, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(256, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(128, num_labels)\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-4)\n",
        "\n",
        "for epoch in range(20):\n",
        "    classifier.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    logits = classifier(combined_features)\n",
        "    loss = criterion(logits, labels.clone().detach())\n",
        "    preds = torch.argmax(logits, dim=1)\n",
        "    correct = (preds == labels).sum().item()\n",
        "    accuracy = correct / labels.size(0)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}, Accuracy = {accuracy:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVHVERFim4TO",
        "outputId": "d583b18f-93a7-4d28-f005-a2397173f448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss = 1.9920, Accuracy = 19.00%\n",
            "Epoch 2: Loss = 1.8690, Accuracy = 22.00%\n",
            "Epoch 3: Loss = 1.8239, Accuracy = 34.00%\n",
            "Epoch 4: Loss = 1.7019, Accuracy = 31.00%\n",
            "Epoch 5: Loss = 1.7073, Accuracy = 31.00%\n",
            "Epoch 6: Loss = 1.6298, Accuracy = 39.00%\n",
            "Epoch 7: Loss = 1.7405, Accuracy = 39.00%\n",
            "Epoch 8: Loss = 1.7225, Accuracy = 44.00%\n",
            "Epoch 9: Loss = 1.5675, Accuracy = 50.00%\n",
            "Epoch 10: Loss = 1.6316, Accuracy = 49.00%\n",
            "Epoch 11: Loss = 1.6549, Accuracy = 49.00%\n",
            "Epoch 12: Loss = 1.6426, Accuracy = 49.00%\n",
            "Epoch 13: Loss = 1.6312, Accuracy = 49.00%\n",
            "Epoch 14: Loss = 1.7825, Accuracy = 46.00%\n",
            "Epoch 15: Loss = 1.6037, Accuracy = 50.00%\n",
            "Epoch 16: Loss = 1.7242, Accuracy = 49.00%\n",
            "Epoch 17: Loss = 1.5511, Accuracy = 50.00%\n",
            "Epoch 18: Loss = 1.6078, Accuracy = 42.00%\n",
            "Epoch 19: Loss = 1.6149, Accuracy = 48.00%\n",
            "Epoch 20: Loss = 1.6569, Accuracy = 45.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits = classifier(combined_features)\n",
        "loss = criterion(logits, labels)"
      ],
      "metadata": {
        "id": "taCii0VP5ch6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize audio features\n",
        "audio_features_norm = (audio_features - audio_features.mean(dim=0)) / (audio_features.std(dim=0) + 1e-6)\n",
        "\n",
        "# Concatenate normalized audio with text embeddings\n",
        "combined_features = torch.cat([text_embeddings, audio_features_norm], dim=1)\n",
        "\n",
        "# Define classifier with hidden layer + ReLU\n",
        "input_dim = combined_features.shape[1]\n",
        "classifier = nn.Sequential(\n",
        "    nn.Linear(input_dim, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, num_labels)\n",
        ")\n",
        "\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-4)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(20):\n",
        "    classifier.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    logits = classifier(combined_features)\n",
        "    loss = criterion(logits, labels)\n",
        "\n",
        "    preds = torch.argmax(logits, dim=1)\n",
        "    correct = (preds == labels).sum().item()\n",
        "    accuracy = correct / labels.size(0)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}, Accuracy = {accuracy:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouJHxg8K8-Z6",
        "outputId": "f2765b31-8257-42c6-b59c-c281f54880ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss = 1.9930, Accuracy = 9.00%\n",
            "Epoch 2: Loss = 1.9618, Accuracy = 9.00%\n",
            "Epoch 3: Loss = 1.9332, Accuracy = 10.00%\n",
            "Epoch 4: Loss = 1.9067, Accuracy = 10.00%\n",
            "Epoch 5: Loss = 1.8815, Accuracy = 37.00%\n",
            "Epoch 6: Loss = 1.8575, Accuracy = 50.00%\n",
            "Epoch 7: Loss = 1.8349, Accuracy = 52.00%\n",
            "Epoch 8: Loss = 1.8137, Accuracy = 53.00%\n",
            "Epoch 9: Loss = 1.7929, Accuracy = 54.00%\n",
            "Epoch 10: Loss = 1.7723, Accuracy = 53.00%\n",
            "Epoch 11: Loss = 1.7516, Accuracy = 53.00%\n",
            "Epoch 12: Loss = 1.7310, Accuracy = 53.00%\n",
            "Epoch 13: Loss = 1.7102, Accuracy = 53.00%\n",
            "Epoch 14: Loss = 1.6892, Accuracy = 53.00%\n",
            "Epoch 15: Loss = 1.6682, Accuracy = 53.00%\n",
            "Epoch 16: Loss = 1.6475, Accuracy = 52.00%\n",
            "Epoch 17: Loss = 1.6271, Accuracy = 52.00%\n",
            "Epoch 18: Loss = 1.6072, Accuracy = 52.00%\n",
            "Epoch 19: Loss = 1.5878, Accuracy = 52.00%\n",
            "Epoch 20: Loss = 1.5692, Accuracy = 52.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To Further investigate, let's run an audio-only training and validation."
      ],
      "metadata": {
        "id": "NsyWkVWciz80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load saved MFCC features\n",
        "with open('/content/drive/MyDrive/DS301-Project/meld_text_audio_features.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "# Pad/truncate MFCCs to fixed length\n",
        "max_len = max(d['audio_mfcc'].shape[1] for d in data)\n",
        "padded_features = []\n",
        "for d in data:\n",
        "    mfcc = torch.tensor(d['audio_mfcc'], dtype=torch.float32)\n",
        "    time_steps = mfcc.shape[1]\n",
        "    if time_steps < max_len:\n",
        "        pad_width = max_len - time_steps\n",
        "        mfcc = F.pad(mfcc, (0, pad_width))\n",
        "    else:\n",
        "        mfcc = mfcc[:, :max_len]\n",
        "    padded_features.append(mfcc)\n",
        "\n",
        "audio_features = torch.stack(padded_features)\n",
        "print(audio_features.shape)\n",
        "\n",
        "# Flatten features for feeding into MLP\n",
        "audio_features_flat = audio_features.view(audio_features.shape[0], -1)\n",
        "input_dim = audio_features_flat.shape[1]\n",
        "\n",
        "# Prepare labels\n",
        "label2id = json.load(open('/content/drive/MyDrive/DS301-Project/code/label2id.json'))\n",
        "labels = torch.tensor([label2id[d['label']] for d in data])\n",
        "\n",
        "num_labels = len(label2id)\n",
        "\n",
        "# Define classifier\n",
        "classifier = nn.Sequential(\n",
        "    nn.Linear(input_dim, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(256, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(128, num_labels)\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-4)\n",
        "\n",
        "# Training\n",
        "for epoch in range(20):\n",
        "    classifier.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    logits = classifier(audio_features_flat)\n",
        "    loss = criterion(logits, labels.clone().detach())\n",
        "\n",
        "    preds = torch.argmax(logits, dim=1)\n",
        "    correct = (preds == labels).sum().item()\n",
        "    accuracy = correct / labels.size(0)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}, Accuracy = {accuracy:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOhBqiiVkwEc",
        "outputId": "754574a3-0751-4f1c-a845-073b1dcfb811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 13, 401])\n",
            "Epoch 1: Loss = 9.0137, Accuracy = 10.00%\n",
            "Epoch 2: Loss = 5.7171, Accuracy = 11.00%\n",
            "Epoch 3: Loss = 4.2682, Accuracy = 32.00%\n",
            "Epoch 4: Loss = 3.5020, Accuracy = 46.00%\n",
            "Epoch 5: Loss = 4.1964, Accuracy = 41.00%\n",
            "Epoch 6: Loss = 3.5363, Accuracy = 42.00%\n",
            "Epoch 7: Loss = 2.7567, Accuracy = 48.00%\n",
            "Epoch 8: Loss = 2.7225, Accuracy = 45.00%\n",
            "Epoch 9: Loss = 2.1840, Accuracy = 48.00%\n",
            "Epoch 10: Loss = 2.0531, Accuracy = 42.00%\n",
            "Epoch 11: Loss = 1.8790, Accuracy = 46.00%\n",
            "Epoch 12: Loss = 1.7845, Accuracy = 42.00%\n",
            "Epoch 13: Loss = 1.5702, Accuracy = 55.00%\n",
            "Epoch 14: Loss = 1.6945, Accuracy = 56.00%\n",
            "Epoch 15: Loss = 1.3058, Accuracy = 56.00%\n",
            "Epoch 16: Loss = 1.2443, Accuracy = 57.00%\n",
            "Epoch 17: Loss = 1.2325, Accuracy = 60.00%\n",
            "Epoch 18: Loss = 1.0825, Accuracy = 64.00%\n",
            "Epoch 19: Loss = 1.1311, Accuracy = 63.00%\n",
            "Epoch 20: Loss = 1.1309, Accuracy = 63.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = audio_features.shape[1] * audio_features.shape[2]\n",
        "\n",
        "audio_only_classifier = nn.Sequential(\n",
        "    nn.Linear(input_dim, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(256, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(128, num_labels)\n",
        ")\n",
        "\n",
        "audio_only_classifier.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Flatten the audio_features for input to linear layer\n",
        "    audio_features_flat = audio_features.view(audio_features.shape[0], -1)\n",
        "\n",
        "    logits = audio_only_classifier(audio_features_flat)\n",
        "    preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "    correct = (preds == labels).sum().item()\n",
        "    total = labels.size(0)\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Audio-only model → Test Accuracy: {accuracy:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gTEKH5xx1ih",
        "outputId": "2b80d43e-acd3-460c-9a9e-53c84ada0482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎵 Audio-only model → Test Accuracy: 13.00%\n"
          ]
        }
      ]
    }
  ]
}